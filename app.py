import asyncio
import os
import csv
import requests
import streamlit as st
from openpyxl import Workbook
from playwright.async_api import async_playwright
import tempfile
from bs4 import BeautifulSoup
import subprocess
import zipfile
import datetime
import random
import pandas as pd
from contextlib import asynccontextmanager

ua_list = []

with open('ua_list.txt', 'r') as f:
    ua_list = f.readlines()

ua_list = [ua.strip() for ua in ua_list]

def ensure_playwright_browser():
    """ç¢ºä¿ Playwright çš„ Chromium ç€è¦½å™¨å·²å®‰è£"""
    browser_path = os.path.expanduser("~/.cache/ms-playwright/chromium-1097/chrome-linux/chrome")
    if not os.path.exists(browser_path):
        print("å®‰è£ Playwright ç€è¦½å™¨...")
        try:
            subprocess.run(["playwright", "install", "chromium"], check=True)
            try:
                subprocess.run(["playwright", "install-deps", "chromium"], check=False)
            except:
                pass
            print("Playwright ç€è¦½å™¨å®‰è£æˆåŠŸ")
        except Exception as e:
            print(f"å®‰è£ Playwright ç€è¦½å™¨å¤±æ•—: {e}")
            try:
                subprocess.run(["python", "-m", "playwright", "install", "chromium"], check=True)
                try:
                    subprocess.run(["python", "-m", "playwright", "install-deps", "chromium"], check=False)
                except:
                    pass
                print("ä½¿ç”¨ python -m å®‰è£ Playwright ç€è¦½å™¨æˆåŠŸ")
            except Exception as e:
                print(f"ä½¿ç”¨ python -m å®‰è£ Playwright ç€è¦½å™¨å¤±æ•—: {e}")

ensure_playwright_browser()

st.set_page_config(
    page_title="è£åˆ¤æ›¸æŸ¥è©¢èˆ‡ä¸‹è¼‰å·¥å…·",
    page_icon="âš–ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

DOWNLOAD_FOLDER = "./downloads"
os.makedirs(DOWNLOAD_FOLDER, exist_ok=True)

@asynccontextmanager
async def get_browser_context():
    """ç€è¦½å™¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    browser = None
    context = None
    try:
        browser = await async_playwright().start()
        browser = await browser.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-gpu',
                '--disable-software-rasterizer',
                '--disable-accelerated-2d-canvas',
                '--no-zygote',
                '--single-process'
            ]
        )
        ua = random.choice(ua_list)
        context = await browser.new_context(
            viewport={"width": 1280, "height": 800},
            user_agent=ua
        )
        yield context
    finally:
        if context:
            await context.close()
        if browser:
            await browser.close()

async def get_judgment_details(context, url):
    """ç²å–è£åˆ¤è©³ç´°è³‡è¨Šï¼ˆå­—è™Ÿã€æ—¥æœŸã€æ¡ˆç”±å’Œè£åˆ¤å…¨æ–‡ï¼‰"""
    page = None
    try:
        page = await context.new_page()
        base_url = "https://judgment.judicial.gov.tw/FJUD/"
        full_url = url if url.startswith("http") else base_url + url
        
        await page.goto(full_url, timeout=30000)
        await page.wait_for_selector(".row", timeout=20000)
        
        rows = await page.query_selector_all(".row")
        case_number = "æœªæ‰¾åˆ°è£åˆ¤å­—è™Ÿ"
        case_date = "æœªæ‰¾åˆ°è£åˆ¤æ—¥æœŸ"
        case_reason = "æœªæ‰¾åˆ°æ¡ˆç”±"
        full_judgment_text = "æœªæ‰¾åˆ°è£åˆ¤å…¨æ–‡"
        
        # Extract case details
        for row in rows:
            text = await row.inner_text()
            if "è£åˆ¤å­—è™Ÿï¼š" in text:
                try:
                    cols = await row.query_selector_all(".col-td")
                    if cols and len(cols) > 0:
                        case_number = await cols[0].inner_text()
                        case_number = case_number.strip()
                except:
                    continue
            elif "è£åˆ¤æ—¥æœŸï¼š" in text:
                try:
                    cols = await row.query_selector_all(".col-td")
                    if cols and len(cols) > 0:
                        case_date = await cols[0].inner_text()
                        case_date = case_date.strip()
                except:
                    continue
            elif "è£åˆ¤æ¡ˆç”±ï¼š" in text:
                try:
                    cols = await row.query_selector_all(".col-td")
                    if cols and len(cols) > 0:
                        case_reason = await cols[0].inner_text()
                        case_reason = case_reason.strip()
                except:
                    continue
        
        # Extract the full judgment text (from a different section)
        judgment_text_element = await page.query_selector(".htmlcontent")
        if judgment_text_element:
            full_judgment_text = await judgment_text_element.inner_text()
            full_judgment_text = full_judgment_text.strip()

        return {
            "case_number": case_number,
            "case_date": case_date,
            "case_reason": case_reason,
            "case_text": full_judgment_text
        }
    except Exception as e:
        print(f"ç²å–è£åˆ¤è©³ç´°è³‡è¨Šå¤±æ•—: {e}")
        return {
            "case_number": "ç²å–å¤±æ•—",
            "case_date": "ç²å–å¤±æ•—",
            "case_reason": "ç²å–å¤±æ•—",
            "case_text": "ç²å–å¤±æ•—"
        }
    finally:
        if page:
            await page.close()

async def fetch_judgments(context, keyword, max_pages=25):
    """éåŒæ­¥ç²å–è£åˆ¤æ›¸è³‡æ–™"""
    progress_placeholder = st.progress(0)
    status_placeholder = st.empty()
    status_placeholder.text("æ­£åœ¨æº–å‚™æŸ¥è©¢...")
    
    try:
        page = await context.new_page()
        
        status_placeholder.text("æ­£åœ¨é€£æ¥æ³•é™¢åˆ¤æ±ºç¶²ç«™...")
        await page.goto("https://judgment.judicial.gov.tw/FJUD/default.aspx", timeout=60000)
        
        status_placeholder.text(f"è¼¸å…¥æœå°‹é—œéµå­—: {keyword}")
        await page.fill("#txtKW", keyword)
        
        status_placeholder.text("é€å‡ºæŸ¥è©¢ï¼Œè«‹ç¨å€™...")
        await page.click("#btnSimpleQry")
        
        await asyncio.sleep(5)
        
        frame = None
        iframe = await page.query_selector("#iframe-data")
        if iframe:
            frame = page.frame(name="iframe-data") or page.frame(id="iframe-data")
        
        if not frame:
            for f in page.frames:
                if "FJUD/data.aspx" in f.url:
                    frame = f
                    break
        
        if not frame:
            frame_locator = page.frame_locator("iframe").first
            if await frame_locator.count() > 0:
                frame = await frame_locator.frame()
        
        if not frame:
            status_placeholder.text("å°‹æ‰¾åˆ¤æ±ºæ¸…å–®æ¡†æ¶...")
            judgment_links = await page.query_selector_all("a[id*='hlTitle']")
            if len(judgment_links) > 0:
                judgment_urls = []
                for link in judgment_links:
                    href = await link.get_attribute("href")
                    text = await link.inner_text()
                    details = await get_judgment_details(context, href)
                    judgment_urls.append({
                        "title": text,
                        "url": href,
                        "case_number": details["case_number"],
                        "case_date": details["case_date"],
                        "case_reason": details["case_reason"],
                        "case_text": details["case_text"]
                    })
                progress_placeholder.progress(1.0)
                status_placeholder.text(f"æ‰¾åˆ° {len(judgment_urls)} ç­†åˆ¤æ±º")
                return judgment_urls, 1
            else:
                progress_placeholder.progress(1.0)
                status_placeholder.text("æœªæ‰¾åˆ°ä»»ä½•åˆ¤æ±º")
                return [], 0
        
        status_placeholder.text("ç­‰å¾…åˆ¤æ±ºæ¸…å–®è¼‰å…¥...")
        await frame.wait_for_selector("a[id*='hlTitle']", timeout=20000)
        
        all_judgments = []
        current_page = 1
        
        while current_page <= max_pages:
            elements = await frame.query_selector_all("a[id*='hlTitle']")
            page_judgments = []
            
            for el in elements:
                url = await el.get_attribute("href")
                text = await el.inner_text()
                details = await get_judgment_details(context, url)
                page_judgments.append({
                    "title": text,
                    "url": url,
                    "case_number": details["case_number"],
                    "case_date": details["case_date"],
                    "case_reason": details["case_reason"],
                    "case_text": details["case_text"]
                })
            
            all_judgments.extend(page_judgments)
            
            progress_percentage = current_page / max_pages
            progress_placeholder.progress(progress_percentage)
            status_placeholder.text(f"é€²åº¦: {current_page}/{max_pages} é  | ç•¶å‰é : {len(page_judgments)}ç­† | ç¸½è¨ˆ: {len(all_judgments)}ç­†")
            
            if current_page < max_pages:
                try:
                    next_link = await frame.query_selector("a#hlNext")
                    if next_link:
                        current_titles = await frame.eval_on_selector_all("a[id*='hlTitle']", "els => els.map(el => el.textContent)")
                        
                        await next_link.click()
                        status_placeholder.text(f"æ­£åœ¨åˆ‡æ›åˆ°ç¬¬ {current_page + 1} é ...")
                        
                        await asyncio.sleep(3)
                        await frame.wait_for_selector("a[id*='hlTitle']", timeout=20000)
                        
                        max_retries = 3
                        for retry in range(max_retries):
                            new_titles = await frame.eval_on_selector_all("a[id*='hlTitle']", "els => els.map(el => el.textContent)")
                            if new_titles != current_titles:
                                break
                            
                            if retry < max_retries - 1:
                                status_placeholder.text(f"ç­‰å¾…é é¢è¼‰å…¥... (å˜—è©¦ {retry + 1}/{max_retries})")
                                await asyncio.sleep(2)
                            else:
                                status_placeholder.warning("é é¢å¯èƒ½æœªæ­£ç¢ºè®ŠåŒ–ï¼Œç¹¼çºŒè™•ç†...")
                    else:
                        status_placeholder.text("å·²åˆ°æœ€å¾Œä¸€é ")
                        break
                except Exception as e:
                    status_placeholder.text(f"åˆ‡æ›é é¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    try:
                        for f in page.frames:
                            if "FJUD/data.aspx" in f.url:
                                frame = f
                                break
                        await asyncio.sleep(3)
                        continue
                    except:
                        break
            
            current_page += 1
        
        total_pages = await get_total_pages(frame)
        
        progress_placeholder.progress(1.0)
        status_placeholder.text(f"å®ŒæˆæŸ¥è©¢ï¼")
        
        return all_judgments, total_pages
        
    except Exception as e:
        progress_placeholder.progress(1.0)
        status_placeholder.text(f"æŸ¥è©¢éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        return [], 0
    finally:
        if page:
            await page.close()

async def get_total_pages(frame):
    """ç²å–ç¸½é æ•¸"""
    try:
        next_link = await frame.query_selector("a#hlNext")
        if not next_link:
            return 1
        
        try:
            page_info = await frame.inner_text("#divPager", timeout=5000)
            if "å…±" in page_info and "é " in page_info:
                parts = page_info.split("å…±")[1].split("é ")[0].strip()
                return int(parts)
        except:
            pass
        
        try:
            select_html = await frame.inner_html("#ddlPage", timeout=5000)
            soup = BeautifulSoup(select_html, 'html.parser')
            select = soup.find('select', {'id': 'ddlPage'})
            
            if select:
                options = select.find_all('option')
                return len(options)
        except:
            pass
        
        return 2
        
    except Exception as e:
        return 1

async def batch_download_pdfs(context, judgment_batch, download_folder, progress_bar=None, status_text=None):
    """æ‰¹é‡ä¸‹è¼‰ä¸€æ‰¹åˆ¤æ±ºæ›¸PDF"""
    downloaded_files = []
    errors = []
    
    for i, judgment in enumerate(judgment_batch):
        name = judgment["case_number"]
        url = judgment["url"]
        
        if status_text:
            status_text.text(f"æ­£åœ¨ä¸‹è¼‰ç¬¬ {i+1}/{len(judgment_batch)} å€‹: {name}")
        if progress_bar:
            progress_bar.progress((i+1)/len(judgment_batch))
        
        file_path, error = await download_judgment_pdf(context, url, download_folder)
        if file_path:
            downloaded_files.append(file_path)
        else:
            errors.append(f"{name}: {error}")
    
    return downloaded_files, errors

async def download_judgment_pdf(context, url, download_folder):
    """ä¸‹è¼‰å–®å€‹è£åˆ¤æ›¸PDF"""
    page = None
    try:
        page = await context.new_page()
        base_url = "https://judgment.judicial.gov.tw/FJUD/"
        full_url = url if url.startswith("http") else base_url + url
        
        await page.goto(full_url, timeout=30000)
        await page.wait_for_selector("#jud", timeout=30000)
        
        # ç²å–è£åˆ¤å­—è™Ÿå’Œæ¡ˆç”±
        rows = await page.query_selector_all("#jud .row")
        case_number = "unknown_case"
        case_reason = "unknown_reason"
        
        for row in rows:
            text = await row.inner_text()
            if "è£åˆ¤å­—è™Ÿï¼š" in text:
                try:
                    cols = await row.query_selector_all(".col-td")
                    if cols and len(cols) > 0:
                        case_number = await cols[0].inner_text()
                        case_number = case_number.strip()
                except:
                    continue
            elif "è£åˆ¤æ¡ˆç”±ï¼š" in text:
                try:
                    cols = await row.query_selector_all(".col-td")
                    if cols and len(cols) > 0:
                        case_reason = await cols[0].inner_text()
                        case_reason = case_reason.strip()
                except:
                    continue
        
        def clean_filename(text):
            keep_chars = (' ', '_', '-', 'ï¼Œ', 'ã€‚', 'ã€', 'ï¼š', 'ï¼›', 'ï¼Ÿ', 'ï¼', 
                         'ã€Œ', 'ã€', 'ã€', 'ã€', 'ï¼ˆ', 'ï¼‰', 'ã€', 'ã€‘', 'ã€Š', 'ã€‹')
            return "".join(c for c in text if c.isalnum() or c in keep_chars).strip()
        
        case_number_clean = clean_filename(case_number)
        case_reason_clean = clean_filename(case_reason)
        
        safe_name = f"{case_number_clean}_{case_reason_clean}.pdf"
        
        if len(safe_name) > 200:
            safe_name = f"{case_number_clean[:150]}_{case_reason_clean[:50]}.pdf"
        
        pdf_link = await page.query_selector("#hlExportPDF")
        if not pdf_link:
            return None, "æ‰¾ä¸åˆ°PDFä¸‹è¼‰é€£çµ"
            
        pdf_url = await pdf_link.get_attribute("href")
        if pdf_url.startswith("/"):
            pdf_url = "https://judgment.judicial.gov.tw" + pdf_url
        
        headers = {
            "User-Agent": random.choice(ua_list)
        }
        response = requests.get(pdf_url, headers=headers)
        
        if response.status_code == 200:
            file_path = os.path.join(download_folder, safe_name)
            with open(file_path, "wb") as f:
                f.write(response.content)
            return file_path, None
        else:
            return None, f"PDFä¸‹è¼‰å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}"
            
    except Exception as e:
        return None, f"ä¸‹è¼‰éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}"
    finally:
        if page:
            await page.close()

def create_excel(judgments):
    """å»ºç«‹Excelæª”æ¡ˆ"""
    wb = Workbook()
    ws = wb.active
    ws.append(["åºè™Ÿ", "è£åˆ¤å­—è™Ÿ", "è£åˆ¤æ—¥æœŸ", "è£åˆ¤æ¡ˆç”±", "åˆ¤æ±ºç¶²å€", "è£åˆ¤æ›¸å…¨æ–‡"])
    
    for idx, judgment in enumerate(judgments, 1):
        ws.append([
            idx,
            judgment["case_number"],
            judgment["case_date"],
            judgment["case_reason"],
            "https://judgment.judicial.gov.tw/FJUD/" + judgment["url"],
            judgment["case_text"]
        ])
    
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx")
    wb.save(temp_file.name)
    temp_file.close()
    
    return temp_file.name

def create_csv(judgments):
    """å»ºç«‹CSVæª”æ¡ˆ"""
    # å‰µå»ºä¸€å€‹è‡¨æ™‚æª”æ¡ˆ
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".csv", mode='w', newline='', encoding='utf-8-sig')
    
    # å®šç¾© CSV å¯«å…¥å™¨
    writer = csv.writer(temp_file)
    
    # å¯«å…¥æ¨™é¡Œ
    writer.writerow(["åºè™Ÿ", "è£åˆ¤å­—è™Ÿ", "è£åˆ¤æ—¥æœŸ", "è£åˆ¤æ¡ˆç”±", "åˆ¤æ±ºç¶²å€", "è£åˆ¤æ›¸å…¨æ–‡"])
    
    # å¯«å…¥æ¯ç­†è£åˆ¤è³‡æ–™
    for idx, judgment in enumerate(judgments, 1):
        writer.writerow([
            idx,
            judgment["case_number"],
            judgment["case_date"],
            judgment["case_reason"],
            "https://judgment.judicial.gov.tw/FJUD/" + judgment["url"],
            judgment["case_text"]
        ])
    
    temp_file.close()
    
    return temp_file.name

with st.sidebar:
    st.markdown("""
    ## é—œæ–¼æœ¬å·¥å…·
                
    æœ¬å·¥å…·ç‚º**å¸æ³•é™¢è£åˆ¤æ›¸æŸ¥è©¢èˆ‡æ‰¹é‡ä¸‹è¼‰å·¥å…·**ï¼Œ
    æ–¹ä¾¿æ‚¨å¿«é€Ÿæœå°‹èˆ‡ä¸‹è¼‰å…¬é–‹è£åˆ¤æ›¸ PDFã€‚
    
    ## ä½¿ç”¨æ•™å­¸
                
    1. è¼¸å…¥æŸ¥è©¢é—œéµå­—  
    2. é¸æ“‡æŸ¥è©¢é æ•¸
    3. é»æ“Šã€Œé–‹å§‹æŸ¥è©¢ã€  
    4. å¯ä¸‹è¼‰ Excelã€CSV æˆ–æ‰¹é‡ä¸‹è¼‰ PDF
                
    ## ğŸ“– æª¢ç´¢å­—è©èªªæ˜äº‹é …
    
    æœ‰é—œæª¢ç´¢å­—è©èªªæ˜ï¼Œè«‹åƒè¦‹[å¸æ³•é™¢è£åˆ¤æ›¸ç³»çµ±](https://judgment.judicial.gov.tw/FJUD/default.aspx)æª¢ç´¢å­—è©è¼”åŠ©èªªæ˜ã€‚é€²å…¥ç¶²é å¾Œæ–¼æœå°‹æ¬„é»æ“Šæœ€å³é‚Šçš„ã€Œæª¢ç´¢å­—è©è¼”åŠ©èªªæ˜ã€å³å¯åƒé–±ã€‚
                
    ## âš ï¸ å…¶ä»–æ³¨æ„äº‹é …
                
    - è‹¥å‡ºç¾éŒ¯èª¤ï¼Œè«‹å¤šæŒ‰å¹¾æ¬¡ã€Œé–‹å§‹æŸ¥è©¢ã€
    - ç”±æ–¼å¸æ³•é™¢è£åˆ¤æ›¸ç³»çµ±é‡å°ä¸€å€‹é—œéµå­—æœ€å¤šåƒ…é¡¯ç¤º 500 ç­†è³‡æ–™ï¼Œå› æ­¤å»ºè­°ä»¥ç²¾ç¢ºé—œéµå­—æœå°‹ï¼ˆå¦‚å¯ä»¥æ–°å¢æ³•é™¢åç¨±ã€åˆ¤æ±ºå¹´ä»½ç­‰ï¼‰
    """)

async def main_async():
    """éåŒæ­¥ä¸»å‡½æ•¸"""
    st.title("âš–ï¸ è£åˆ¤æ›¸æŸ¥è©¢èˆ‡ä¸‹è¼‰å·¥å…·")
    st.markdown("""
        æœ¬å·¥å…·å¯æŸ¥è©¢å¸æ³•é™¢è£åˆ¤æ›¸ç³»çµ±ï¼Œä¸¦ä¸‹è¼‰ç›¸é—œè£åˆ¤æ›¸PDFæª”æ¡ˆã€‚
        è«‹è¼¸å…¥æŸ¥è©¢é—œéµå­—ï¼Œç„¶å¾Œé»æ“Šã€ŒæŸ¥è©¢ã€æŒ‰éˆ•ã€‚
    """)
    
    # åˆå§‹åŒ– session state
    if "search_clicked" not in st.session_state:
        st.session_state.search_clicked = False
    if "judgments" not in st.session_state:
        st.session_state.judgments = []
    if "csv_file" not in st.session_state:
        st.session_state.csv_file = None
    if "excel_file" not in st.session_state:
        st.session_state.excel_file = None
    if "download_all" not in st.session_state:
        st.session_state.download_all = False
    if "batch_download" not in st.session_state:
        st.session_state.batch_download = False
    if "total_pages" not in st.session_state:
        st.session_state.total_pages = 1
    if "download_progress" not in st.session_state:
        st.session_state.download_progress = 0
    if "search_completed" not in st.session_state:
        st.session_state.search_completed = False
    if "current_display_page" not in st.session_state:
        st.session_state.current_display_page = 1
    
    keyword = st.text_input(
        "æŸ¥è©¢é—œéµå­—", 
        value="(æ³•é™¢+ç®¡è½„)&å…¬è­‰è™•",
        help="ä½¿ç”¨é€²éšæŸ¥è©¢èªæ³•ï¼Œä¾‹å¦‚ï¼š(ä¿éšªå…¬å¸&åŸ·è¡Œå‘½ä»¤)&æœ€é«˜"
    )

    max_pages = st.number_input(
        "æŸ¥è©¢é æ•¸", 
        min_value=1, 
        max_value=25, 
        value=1,
        help="è¨­å®šè¦æŸ¥è©¢çš„é æ•¸ï¼ˆæ¯é ç´„20ç­†çµæœï¼Œæœ€å¤š25é ï¼‰"
    )

    if "search_clicked" not in st.session_state:
        st.session_state.search_clicked = False

    if st.button("é–‹å§‹æŸ¥è©¢"):
        st.session_state.search_clicked = True
        st.session_state.search_completed = False
        st.session_state.download_all = False
        st.session_state.judgments = []
        st.session_state.excel_file = None
        st.session_state.csv_file = None
    
    async with get_browser_context() as context:
        if st.session_state.get("search_clicked", False):
            search_result_container = st.container()
            with search_result_container:
                if not st.session_state.get("search_completed", False):
                    with st.spinner("æ­£åœ¨æŸ¥è©¢è£åˆ¤æ›¸ï¼Œè«‹ç¨å€™..."):
                        judgments, total_pages = await fetch_judgments(context, keyword, max_pages)
                        st.session_state.total_pages = total_pages
                        st.session_state.current_display_page = 1  # é‡ç½®ç‚ºç¬¬ä¸€é 
                        
                        if not judgments:
                            st.warning("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„è£åˆ¤æ›¸ï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµå­—ã€‚")
                            st.session_state.search_clicked = False
                            return
                        
                        st.session_state.judgments = judgments
                        st.session_state.search_completed = True
                
                result_count = len(st.session_state.judgments)
                # st.success(f"æ‰¾åˆ° {result_count} ç­†è£åˆ¤æ›¸çµæœ")
                
                excel_file = create_excel(st.session_state.judgments)
                csv_file = create_csv(st.session_state.judgments)
                st.session_state.excel_file = excel_file
                st.session_state.csv_file = csv_file
                
                results_container = st.container()
                with results_container:
                    st.subheader("æŸ¥è©¢çµæœ")
                    
                    items_per_page = 20
                    total_items = len(st.session_state.judgments)
                    total_result_pages = (total_items + items_per_page - 1) // items_per_page
                    
                    st.write(f"ç•¶å‰é ç¢¼: {st.session_state.current_display_page}/{total_result_pages}")
                    
                    # åˆ†é æ§åˆ¶æŒ‰éˆ•
                    col1, col2, col3, col4, col5 = st.columns(5)
                    with col1:
                        if st.button("ç¬¬ä¸€é ", disabled=st.session_state.current_display_page == 1):
                            st.session_state.current_display_page = 1
                    with col2:
                        if st.button("ä¸Šä¸€é ", disabled=st.session_state.current_display_page == 1):
                            st.session_state.current_display_page -= 1
                    with col3:
                        st.write("")  # ç©ºç™½åˆ—ç”¨æ–¼é–“éš”
                    with col4:
                        if st.button("ä¸‹ä¸€é ", disabled=st.session_state.current_display_page == total_result_pages):
                            st.session_state.current_display_page += 1
                    with col5:
                        if st.button("æœ€å¾Œä¸€é ", disabled=st.session_state.current_display_page == total_result_pages):
                            st.session_state.current_display_page = total_result_pages
                    
                    start_idx = (st.session_state.current_display_page - 1) * items_per_page
                    end_idx = min(start_idx + items_per_page, total_items)
                    
                    st.info(f"é¡¯ç¤ºç¬¬ {start_idx+1}-{end_idx} ç­†ï¼ˆå…± {total_items} ç­†ï¼‰")
                    
                    current_page_judgments = st.session_state.judgments[start_idx:end_idx]
                    
                    table_data = []
                    for idx, judgment in enumerate(current_page_judgments, start_idx + 1):
                        table_data.append({
                            "åºè™Ÿ": idx,
                            "è£åˆ¤å­—è™Ÿ": judgment["case_number"],
                            "è£åˆ¤æ—¥æœŸ": judgment["case_date"],
                            "è£åˆ¤æ¡ˆç”±": judgment["case_reason"]
                        })
                    
                    df = pd.DataFrame(table_data)
                    df = df.reset_index(drop=True)
                    st.table(df.style.hide(axis="index"))
                    
                    if st.button(f"ä¸‹è¼‰ç•¶å‰é  PDFï¼ˆ{len(current_page_judgments)} ç­†ï¼‰"):
                        st.session_state.batch_download = True
                        st.session_state.batch_judgments = current_page_judgments
                
                st.subheader("æ‰¹é‡ä¸‹è¼‰é¸é …")
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    if st.button("ä¸‹è¼‰æ‰€æœ‰æŸ¥è©¢çµæœ PDF (ZIP)"):
                        st.session_state.download_all = True
                
                with col2:
                    st.download_button(
                        label="ä¸‹è¼‰æŸ¥è©¢çµæœæ¸…å–® (Excel)",
                        data=open(st.session_state.excel_file, "rb"),
                        file_name=f"{keyword}_è£åˆ¤æ›¸æŸ¥è©¢çµæœ.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )

                with col3:
                    st.download_button(
                        label="ä¸‹è¼‰æŸ¥è©¢çµæœæ¸…å–® (csv)",
                        data=open(st.session_state.csv_file, "rb"),
                        file_name=f"{keyword}_è£åˆ¤æ›¸æŸ¥è©¢çµæœ.csv",
                        mime="text/csv"
                    )
        
        if st.session_state.get("batch_download", False) and st.session_state.get("batch_judgments"):
            judgments_batch = st.session_state.batch_judgments
            total = len(judgments_batch)
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            status_text.text(f"æº–å‚™ä¸‹è¼‰ {total} ç­†åˆ¤æ±ºæ–‡ä»¶...")
            
            temp_dir = tempfile.mkdtemp()
            
            with st.spinner(f"æ­£åœ¨ä¸‹è¼‰ {total} ç­†åˆ¤æ±º..."):
                downloaded_files, errors = await batch_download_pdfs(context, judgments_batch, temp_dir, progress_bar, status_text)
            
            if downloaded_files:
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                zip_filename = f"è£åˆ¤æ›¸åˆé›†_{timestamp}.zip"
                zip_path = os.path.join(temp_dir, zip_filename)
                
                with zipfile.ZipFile(zip_path, 'w') as zipf:
                    for file in downloaded_files:
                        zipf.write(file, os.path.basename(file))
                
                with open(zip_path, "rb") as f:
                    st.download_button(
                        label=f"é»æ“Šä¸‹è¼‰ {len(downloaded_files)} ç­†è£åˆ¤æ›¸ (ZIP)",
                        data=f,
                        file_name=zip_filename,
                        mime="application/zip"
                    )
                
                st.success(f"å·²æˆåŠŸä¸‹è¼‰ {len(downloaded_files)}/{total} å€‹è£åˆ¤æ›¸")
                
                if errors:
                    st.warning("éƒ¨åˆ†è£åˆ¤æ›¸ä¸‹è¼‰å¤±æ•—:")
                    for error in errors:
                        st.error(error)
            else:
                st.error("æ²’æœ‰ä»»ä½•è£åˆ¤æ›¸ä¸‹è¼‰æˆåŠŸ")
            
            for file in downloaded_files:
                if os.path.exists(file):
                    os.remove(file)
            if os.path.exists(zip_path):
                os.remove(zip_path)
            os.rmdir(temp_dir)
            
            st.session_state.batch_download = False
            st.session_state.batch_judgments = None
        
        if st.session_state.get("download_all", False) and st.session_state.get("judgments"):
            judgments = st.session_state.judgments
            total = len(judgments)
            
            download_progress = st.progress(0)
            download_status = st.empty()
            download_status.text(f"æº–å‚™ä¸‹è¼‰ {total} ç­†åˆ¤æ±ºæ–‡ä»¶...")
            
            temp_dir = tempfile.mkdtemp()
            downloaded_files = []
            download_errors = []
            
            with st.spinner(f"æ­£åœ¨ä¸‹è¼‰å…¨éƒ¨ {total} ç­†åˆ¤æ±º..."):
                downloaded_files, download_errors = await batch_download_pdfs(context, judgments, temp_dir, download_progress, download_status)
            
            if downloaded_files:
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                zip_filename = f"è£åˆ¤æ›¸åˆé›†_å…¨éƒ¨_{timestamp}.zip"
                zip_path = os.path.join(temp_dir, zip_filename)
                
                with zipfile.ZipFile(zip_path, 'w') as zipf:
                    for file in downloaded_files:
                        zipf.write(file, os.path.basename(file))
                
                with open(zip_path, "rb") as f:
                    st.download_button(
                        label=f"é»æ“Šä¸‹è¼‰æ‰€æœ‰è£åˆ¤æ›¸ (ZIP)",
                        data=f,
                        file_name=zip_filename,
                        mime="application/zip"
                    )
                
                st.success(f"å·²æˆåŠŸä¸‹è¼‰ {len(downloaded_files)}/{total} å€‹è£åˆ¤æ›¸")
                
                if download_errors:
                    st.warning("éƒ¨åˆ†è£åˆ¤æ›¸ä¸‹è¼‰å¤±æ•—:")
                    for error in download_errors:
                        st.error(error)
            else:
                st.error("æ²’æœ‰ä»»ä½•è£åˆ¤æ›¸ä¸‹è¼‰æˆåŠŸ")
            
            for file in downloaded_files:
                if os.path.exists(file):
                    os.remove(file)
            if os.path.exists(zip_path):
                os.remove(zip_path)
            os.rmdir(temp_dir)
            
            st.session_state.download_all = False

def main():
    asyncio.run(main_async())

if __name__ == "__main__":
    main()